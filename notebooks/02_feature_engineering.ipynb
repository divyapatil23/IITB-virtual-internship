{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import skew, kurtosis\n",
        "import os\n",
        "\n",
        "# --- Configuration ---\n",
        "INPUT_EEG_FILE = 'extracted_EEG_data.csv'\n",
        "OUTPUT_EEG_FEATURES = 'T1_G24_EEG_features.csv'\n",
        "\n",
        "def extract_eeg_features_from_file(input_file, output_file):\n",
        "    \"\"\"Computes band power statistics (mean, std, skew, kurtosis) per trial.\"\"\"\n",
        "    print(f\"--- Processing EEG data for Feature Engineering ---\")\n",
        "    try:\n",
        "        # Load the synchronized/extracted EEG data\n",
        "        df_eeg = pd.read_csv(input_file)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: {input_file} not found. Skipping EEG feature extraction.\")\n",
        "        return\n",
        "\n",
        "    if 'Trial_Index' not in df_eeg.columns:\n",
        "        print(\"Error: 'Trial_Index' column is missing. Cannot group by trial.\")\n",
        "        return\n",
        "\n",
        "    # Identify columns containing band power values (assuming they are numeric and prefixed)\n",
        "    # Adjust this list based on the actual columns in your extracted file (e.g., Alpha, Beta, etc.)\n",
        "    band_cols = [col for col in df_eeg.columns if col not in ['Trial_Index', 'Timestamp', 'Participant_ID']]\n",
        "\n",
        "    def extract_stats(series, prefix):\n",
        "        \"\"\"Helper to compute statistics on a single band power series.\"\"\"\n",
        "        features = {}\n",
        "        if not series.empty:\n",
        "            features[f'{prefix}_mean'] = series.mean()\n",
        "            features[f'{prefix}_std'] = series.std()\n",
        "            features[f'{prefix}_max'] = series.max()\n",
        "            features[f'{prefix}_min'] = series.min()\n",
        "            features[f'{prefix}_skew'] = skew(series.dropna())\n",
        "            features[f'{prefix}_kurt'] = kurtosis(series.dropna())\n",
        "        return pd.Series(features)\n",
        "\n",
        "    all_eeg_features = []\n",
        "\n",
        "    # Iterate through each trial and extract features\n",
        "    for trial_index, df_trial in df_eeg.groupby('Trial_Index'):\n",
        "        trial_features = {'Trial_Index': trial_index}\n",
        "\n",
        "        # Optionally, preserve a key identifier like Participant_ID\n",
        "        if 'Participant_ID' in df_trial.columns:\n",
        "            trial_features['Participant_ID'] = df_trial['Participant_ID'].iloc[0]\n",
        "\n",
        "        # Extract features for each band/channel\n",
        "        for col in band_cols:\n",
        "            stats = extract_stats(df_trial[col], col)\n",
        "            trial_features.update(stats)\n",
        "\n",
        "        all_eeg_features.append(trial_features)\n",
        "\n",
        "    df_features = pd.DataFrame(all_eeg_features)\n",
        "    df_features.to_csv(output_file, index=False)\n",
        "    print(f\"-> Successfully saved EEG features to {output_file}\")\n",
        "    print(df_features.head())\n",
        "\n",
        "# extract_eeg_features_from_file(INPUT_EEG_FILE, OUTPUT_EEG_FEATURES)"
      ],
      "metadata": {
        "id": "UCqqXjc4RET9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.signal import find_peaks\n",
        "\n",
        "INPUT_GSR_FILE = 'extracted_GSR_data.csv'\n",
        "OUTPUT_GSR_FEATURES = 'T1_G24_GSR_features.csv'\n",
        "\n",
        "def extract_gsr_features_from_file(input_file, output_file):\n",
        "    \"\"\"Computes GSR features like arousal slope and peak counts per trial.\"\"\"\n",
        "    print(f\"\\n--- Processing GSR data for Feature Engineering ---\")\n",
        "    try:\n",
        "        df_gsr = pd.read_csv(input_file)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: {input_file} not found. Skipping GSR feature extraction.\")\n",
        "        return\n",
        "\n",
        "    if 'Trial_Index' not in df_gsr.columns:\n",
        "        print(\"Error: 'Trial_Index' column is missing. Cannot group by trial.\")\n",
        "        return\n",
        "\n",
        "    # Assuming the arousal data is in a column named 'GSR_Value' or similar\n",
        "    GSR_COLUMN = [col for col in df_gsr.columns if 'GSR' in col or 'arousal' in col]\n",
        "    if not GSR_COLUMN:\n",
        "        print(\"Error: Could not identify GSR value column (looking for 'GSR' or 'arousal').\")\n",
        "        return\n",
        "    GSR_COLUMN = GSR_COLUMN[0]\n",
        "\n",
        "    def extract_gsr_stats(df_trial):\n",
        "        \"\"\"Helper to compute features for a single trial.\"\"\"\n",
        "        series = df_trial[GSR_COLUMN].dropna()\n",
        "        features = {}\n",
        "\n",
        "        if not series.empty:\n",
        "            # 1. Slope of Arousal (Approximation using first/last point difference)\n",
        "            features['GSR_Arousal_Slope'] = (series.iloc[-1] - series.iloc[0]) / len(series)\n",
        "\n",
        "            # 2. Peak Counts per Window (Requires a minimum peak height/distance)\n",
        "            # Find peaks: height=threshold (e.g., mean of data), distance=min_samples_between_peaks\n",
        "            peaks, _ = find_peaks(series, height=series.mean() * 0.8, distance=TARGET_RATE_HZ * 2)\n",
        "            features['GSR_Peak_Count'] = len(peaks)\n",
        "\n",
        "            # Additional features\n",
        "            features['GSR_Mean'] = series.mean()\n",
        "            features['GSR_Variance'] = series.var()\n",
        "\n",
        "        return pd.Series(features)\n",
        "\n",
        "    df_features = df_gsr.groupby('Trial_Index').apply(extract_gsr_stats).reset_index()\n",
        "    df_features.to_csv(output_file, index=False)\n",
        "    print(f\"-> Successfully saved GSR features to {output_file}\")\n",
        "    print(df_features.head())\n",
        "\n",
        "# extract_gsr_features_from_file(INPUT_GSR_FILE, OUTPUT_GSR_FEATURES)"
      ],
      "metadata": {
        "id": "4yi-WjzdUtzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_eeg_features_from_file(input_file, output_file):\n",
        "    \"\"\"Computes band power statistics (mean, std, skew, kurtosis) per trial.\"\"\"\n",
        "    print(f\"--- Processing EEG data for Feature Engineering ---\")\n",
        "    try:\n",
        "        # Load the synchronized/extracted EEG data\n",
        "        # --- REVISED LINE ---\n",
        "        # 1. Try a common alternative delimiter (e.g., semicolon) if applicable.\n",
        "        # 2. Use 'engine=\"python\"' as it handles irregular lines better than the C engine.\n",
        "        df_eeg = pd.read_csv(\n",
        "            input_file,\n",
        "            # sep=',', # Keep comma by default, or change to ';' if you suspect it\n",
        "            engine='python',\n",
        "            on_bad_lines='warn' # 'warn' prints a message but attempts to skip or fix, 'skip' ignores the bad line entirely\n",
        "        )\n",
        "        # --- END REVISED LINE ---\n",
        "\n",
        "        # If the delimiter is the issue, you might need to test:\n",
        "        # df_eeg = pd.read_csv(input_file, sep=';', engine='python', on_bad_lines='warn')\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: {input_file} not found. Skipping EEG feature extraction.\")\n",
        "        return\n",
        "    # ... rest of the function remains the same ..."
      ],
      "metadata": {
        "id": "NWX3rKqWV_hy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import skew, kurtosis\n",
        "from scipy.signal import find_peaks"
      ],
      "metadata": {
        "id": "j6-V6K92da9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Mount Google Drive (Run this cell first!)\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Define your project's root folder structure (ADJUST THIS PATH!)\n",
        "PROJECT_ROOT = '/content/drive/MyDrive/emotion_clustering/'\n",
        "DATA_DIR = os.path.join(PROJECT_ROOT, 'data')\n",
        "MODELS_DIR = os.path.join(PROJECT_ROOT, 'models')\n",
        "\n",
        "# Create directories (if they don't exist)\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"Project files will be read from/saved to: {PROJECT_ROOT}\")\n",
        "\n",
        "# --- Global Configurations ---\n",
        "# Target downsample rate from Step 2.1\n",
        "TARGET_RATE_HZ = 5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mh237RJrdev8",
        "outputId": "b3ea0616-a9fa-43cc-854e-5ed8ec493cc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Project files will be read from/saved to: /content/drive/MyDrive/emotion_clustering/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_EEG_FILE = 'extracted_EEG_data.csv'\n",
        "OUTPUT_EEG_FEATURES = os.path.join(PROJECT_ROOT, 'T1_G24_EEG_features.csv')\n",
        "\n",
        "def extract_eeg_features_from_file(input_file, output_file):\n",
        "    print(f\"\\n--- Processing EEG data: {input_file} ---\")\n",
        "    try:\n",
        "        # Using engine='python' and on_bad_lines='skip' to handle potential errors from Step 2.1 output\n",
        "        df_eeg = pd.read_csv(input_file, engine='python', on_bad_lines='skip')\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {input_file}: {e}\")\n",
        "        return\n",
        "\n",
        "    if 'Trial_Index' not in df_eeg.columns:\n",
        "        print(\"Error: 'Trial_Index' column is missing.\")\n",
        "        return\n",
        "\n",
        "    band_cols = [col for col in df_eeg.columns if col not in ['Trial_Index', 'Timestamp', 'Participant_ID']]\n",
        "\n",
        "    def extract_stats(series, prefix):\n",
        "        features = {}\n",
        "        series = series.dropna()\n",
        "        if not series.empty:\n",
        "            features[f'{prefix}_mean'] = series.mean()\n",
        "            features[f'{prefix}_std'] = series.std()\n",
        "            features[f'{prefix}_skew'] = skew(series)\n",
        "            features[f'{prefix}_kurt'] = kurtosis(series)\n",
        "        return pd.Series(features)\n",
        "\n",
        "    all_eeg_features = []\n",
        "    for trial_index, df_trial in df_eeg.groupby('Trial_Index'):\n",
        "        trial_features = {'Trial_Index': trial_index}\n",
        "\n",
        "        # Extract features for each band/channel\n",
        "        for col in band_cols:\n",
        "            stats = extract_stats(df_trial[col], col)\n",
        "            trial_features.update(stats)\n",
        "\n",
        "        all_eeg_features.append(trial_features)\n",
        "\n",
        "    df_features = pd.DataFrame(all_eeg_features)\n",
        "    df_features.to_csv(output_file, index=False)\n",
        "    print(f\"-> Saved EEG features to {output_file}\")\n",
        "\n",
        "# extract_eeg_features_from_file(INPUT_EEG_FILE, OUTPUT_EEG_FEATURES) # RUN THIS"
      ],
      "metadata": {
        "id": "VfN2u-tueVSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_GSR_FILE = 'extracted_GSR_data.csv'\n",
        "OUTPUT_GSR_FEATURES = os.path.join(PROJECT_ROOT, 'T1_G24_GSR_features.csv')\n",
        "\n",
        "def extract_gsr_features_from_file(input_file, output_file):\n",
        "    print(f\"\\n--- Processing GSR data: {input_file} ---\")\n",
        "    try:\n",
        "        df_gsr = pd.read_csv(input_file, engine='python', on_bad_lines='skip')\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {input_file}: {e}\")\n",
        "        return\n",
        "\n",
        "    if 'Trial_Index' not in df_gsr.columns:\n",
        "        print(\"Error: 'Trial_Index' column is missing.\")\n",
        "        return\n",
        "\n",
        "    # Assuming 'GSR_Value' is the column name for arousal\n",
        "    GSR_COLUMN = 'GSR_Value'\n",
        "    if GSR_COLUMN not in df_gsr.columns:\n",
        "        # Try to infer a GSR column if 'GSR_Value' doesn't exist\n",
        "        potential_cols = [col for col in df_gsr.columns if 'GSR' in col or 'arousal' in col and col not in ['Trial_Index', 'Timestamp']]\n",
        "        if potential_cols:\n",
        "             GSR_COLUMN = potential_cols[0]\n",
        "        else:\n",
        "             print(f\"Error: Could not find a suitable GSR column.\")\n",
        "             return\n",
        "\n",
        "    def extract_gsr_stats(df_trial):\n",
        "        series = df_trial[GSR_COLUMN].dropna()\n",
        "        features = {}\n",
        "        if len(series) > 1:\n",
        "            # Slope of Arousal\n",
        "            features['GSR_Arousal_Slope'] = (series.iloc[-1] - series.iloc[0]) / len(series)\n",
        "\n",
        "            # Peak Counts (distance ensures minimum separation between peaks)\n",
        "            # Find peaks: height=threshold (e.g., 0.8 * mean), distance=min_samples_between_peaks\n",
        "            peaks, _ = find_peaks(series, height=series.mean() * 0.8, distance=TARGET_RATE_HZ * 2)\n",
        "            features['GSR_Peak_Count'] = len(peaks)\n",
        "\n",
        "            features['GSR_Mean'] = series.mean()\n",
        "\n",
        "        return pd.Series(features)\n",
        "\n",
        "    df_features = df_gsr.groupby('Trial_Index').apply(extract_gsr_stats).reset_index()\n",
        "    df_features.to_csv(output_file, index=False)\n",
        "    print(f\"-> Saved GSR features to {output_file}\")\n",
        "\n",
        "# extract_gsr_features_from_file(INPUT_GSR_FILE, OUTPUT_GSR_FEATURES) # RUN THIS"
      ],
      "metadata": {
        "id": "y22xWRi8evUZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}